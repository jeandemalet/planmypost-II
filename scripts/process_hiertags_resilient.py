import pandas as pd
import json
import os

def process_hiertags_resilient(
    input_filename="flickr_tag_co-occurrence_network.tsv", 
    output_filename="hiertags_relations_raw.jsonl",  # Format .jsonl
    progress_file="hiertags_progress.txt",
    min_weight=0.1,
    chunk_size=100000  # Traiter 100 000 lignes √† la fois
):
    """
    Analyse le fichier HIERTAGS de mani√®re r√©siliente, en sauvegardant la progression.
    """
    start_line = 0
    
    # Logique de reprise
    if os.path.exists(progress_file):
        with open(progress_file, 'r') as f:
            try:
                start_line = int(f.read().strip())
                print(f"üîÑ Reprise du traitement √† partir de la ligne {start_line}...")
            except ValueError:
                print("‚ö†Ô∏è Fichier de progression invalide, recommencement.")
                start_line = 0
    else:
        # Si le fichier de sortie existe mais pas le fichier de progression, on nettoie
        if os.path.exists(output_filename):
            os.remove(output_filename)
            print("üßπ Nettoyage du fichier de sortie pr√©c√©dent.")

    try:
        print(f"üìä Chargement du jeu de donn√©es '{input_filename}' par lots...")
        
        # Lecture par chunks pour √©conomiser la m√©moire
        iterator = pd.read_csv(
            input_filename, 
            sep='\t', 
            header=None, 
            names=['tag1', 'tag2', 'weight'],
            chunksize=chunk_size,
            skiprows=range(1, start_line + 1) if start_line > 0 else None
        )
        
        processed_count = start_line
        
        # √âcriture en mode append
        with open(output_filename, 'a', encoding='utf-8') as f:
            for i, chunk in enumerate(iterator):
                # Filtrer par poids minimum
                chunk_filtered = chunk[chunk['weight'] >= min_weight]
                
                for row in chunk_filtered.itertuples(index=False):
                    tag1, tag2, weight = str(row.tag1), str(row.tag2), float(row.weight)
                    
                    # √âcrire une ligne JSON pour chaque relation (bidirectionnelle)
                    relation1 = {"tag": tag1, "related": tag2, "weight": weight}
                    relation2 = {"tag": tag2, "related": tag1, "weight": weight}
                    
                    f.write(json.dumps(relation1, ensure_ascii=False) + '\n')
                    f.write(json.dumps(relation2, ensure_ascii=False) + '\n')
                
                processed_count += len(chunk)
                
                # Sauvegarde de la progression
                with open(progress_file, 'w') as pf:
                    pf.write(str(processed_count))
                
                print(f"  ‚úÖ Lot {i+1} trait√©. Total de lignes analys√©es : {processed_count:,}")
                
    except FileNotFoundError:
        print(f"‚ùå ERREUR: Fichier '{input_filename}' non trouv√©.")
        print("T√©l√©chargez-le depuis : https://www.ims.uni-stuttgart.de/en/research/resources/corpora/HierTags/")
        return
    except KeyboardInterrupt:
        print(f"\n‚è∏Ô∏è Interruption d√©tect√©e. Progression sauvegard√©e jusqu'√† la ligne {processed_count}.")
        print("Vous pouvez relancer le script pour reprendre le traitement.")
        return
    except Exception as e:
        print(f"‚ùå Une erreur est survenue : {e}")
        print(f"La progression est sauvegard√©e jusqu'√† la ligne {processed_count}. Vous pouvez relancer le script.")
        return
    
    # Nettoyage des fichiers temporaires en cas de succ√®s
    if os.path.exists(progress_file):
        os.remove(progress_file)
        print("üßπ Fichier de progression supprim√© (traitement termin√©).")
    
    print(f"\nüéâ Traitement termin√© avec succ√®s !")
    print(f"üìÅ Donn√©es sauvegard√©es dans '{output_filename}'")

if __name__ == "__main__":
    process_hiertags_resilient()